"""
AI Style Extraction Module
Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
"""

import logging
import re
from typing import List, Optional
from collections import Counter

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ SentenceTransformer
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    SENTENCE_TRANSFORMER_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMER_AVAILABLE = False
    logging.warning("SentenceTransformer Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ - Ğ±ÑƒĞ´ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ°")

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ sklearn Ğ´Ğ»Ñ cosine similarity
try:
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ transformers Ğ¸ torch Ğ´Ğ»Ñ LLM
try:
    import torch
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("Transformers Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ - generate_reply Ğ±ÑƒĞ´ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ fallback")

logger = logging.getLogger(__name__)


# Ğ¤Ñ€Ğ°Ğ·Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ„Ğ»Ğ¸Ñ€Ñ‚ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾Ğ½Ğ°
FLIRTY_PHRASES = [
    "you're so hot",
    "you're beautiful",
    "you're sexy",
    "I want you",
    "I need you",
    "you turn me on",
    "you're amazing",
    "you're gorgeous",
    "I love",
    "you're perfect",
    "so hot",
    "so sexy",
    "so beautiful",
    "ğŸ’•",
    "ğŸ˜˜",
    "ğŸ¥°",
    "ğŸ˜",
    "ğŸ”¥",
    "ğŸ’–",
    "â¤ï¸",
    "ğŸ’‹",
    "kiss",
    "hug",
    "cuddle",
    "baby",
    "babe",
    "sweetheart",
    "honey",
    "darling"
]


def extract_emojis(text: str) -> List[str]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
    # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ Ğ´Ğ»Ñ ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸ (Unicode ranges)
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # Emoticons
        "\U0001F300-\U0001F5FF"  # Symbols & Pictographs
        "\U0001F680-\U0001F6FF"  # Transport & Map
        "\U0001F1E0-\U0001F1FF"  # Flags
        "\U00002702-\U000027B0"  # Dingbats
        "\U000024C2-\U0001F251"  # Enclosed characters
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.findall(text)


def extract_style(replies: List[str]) -> str:
    """
    Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²
    
    Args:
        replies: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑÑ‚Ñ€Ğ¾Ğº Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        
    Returns:
        Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ° Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¸Ğ»Ñ (style_desc)
    """
    if not replies:
        logger.warning("ĞĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ‚Ğ¸Ğ»Ñ")
        return "No style data available"
    
    logger.info(f"ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¸Ğ· {len(replies)} Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²...")
    
    # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ²ÑĞµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ğ¾Ğ´Ğ¸Ğ½ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
    all_text = " ".join(replies).lower()
    
    # 1. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· SentenceTransformer (cosine similarity Ñ flirty phrases)
    tone_score = 0.0
    tone_description = "neutral"
    
    if SENTENCE_TRANSFORMER_AVAILABLE and SKLEARN_AVAILABLE:
        try:
            logger.info("Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ SentenceTransformer Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ğ¾Ğ½Ğ°...")
            model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # Ğ­Ğ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ñ„Ğ»Ğ¸Ñ€Ñ‚ÑƒÑÑ‰Ğ¸Ñ… Ñ„Ñ€Ğ°Ğ·
            flirty_embeddings = model.encode(FLIRTY_PHRASES)
            
            # Ğ­Ğ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² (Ğ±ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 50 Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸)
            sample_replies = replies[:50]
            reply_embeddings = model.encode(sample_replies)
            
            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ cosine similarity
            similarities = cosine_similarity(reply_embeddings, flirty_embeddings)
            max_similarities = similarities.max(axis=1)
            tone_score = float(max_similarities.mean())
            
            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ score
            if tone_score > 0.5:
                tone_description = "very flirty and romantic"
            elif tone_score > 0.4:
                tone_description = "flirty and playful"
            elif tone_score > 0.3:
                tone_description = "friendly and warm"
            elif tone_score > 0.2:
                tone_description = "casual and friendly"
            else:
                tone_description = "professional and neutral"
            
            logger.info(f"Tone score: {tone_score:.3f}, Description: {tone_description}")
            
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ‚Ğ¾Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· SentenceTransformer: {e}")
            # Fallback Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·
            pass
    
    # Fallback: Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ğ½Ğ° Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼
    if tone_score == 0.0:
        flirty_count = sum(1 for phrase in FLIRTY_PHRASES if phrase in all_text)
        total_words = len(all_text.split())
        flirty_ratio = flirty_count / max(total_words, 1) * 100
        
        if flirty_ratio > 5:
            tone_description = "very flirty and romantic"
        elif flirty_ratio > 3:
            tone_description = "flirty and playful"
        elif flirty_ratio > 1:
            tone_description = "friendly and warm"
        else:
            tone_description = "casual and professional"
        
        logger.info(f"Basic tone analysis: {flirty_ratio:.2f}% flirty phrases")
    
    # 2. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Counter
    all_emojis = []
    for reply in replies:
        emojis = extract_emojis(reply)
        all_emojis.extend(emojis)
    
    emoji_counter = Counter(all_emojis)
    top_emojis = emoji_counter.most_common(10)
    
    emoji_description = ""
    if top_emojis:
        emoji_list = ", ".join([f"{emoji} ({count})" for emoji, count in top_emojis[:5]])
        emoji_description = f"Top emojis: {emoji_list}"
        logger.info(f"Top emojis: {emoji_list}")
    else:
        emoji_description = "No emojis used"
        logger.info("Ğ­Ğ¼Ğ¾Ğ´Ğ·Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
    
    # 3. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ğ¸Ğ½Ñ‹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
    avg_length = sum(len(reply) for reply in replies) / len(replies)
    if avg_length > 200:
        length_description = "long and detailed"
    elif avg_length > 100:
        length_description = "medium length"
    else:
        length_description = "short and concise"
    
    logger.info(f"Average message length: {avg_length:.1f} characters")
    
    # 4. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ±ÑƒĞºĞ² (ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ)
    caps_ratio = sum(1 for reply in replies if any(c.isupper() for c in reply[:10])) / len(replies)
    if caps_ratio > 0.3:
        caps_description = "enthusiastic (uses caps)"
    else:
        caps_description = "calm and composed"
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ
    style_desc = (
        f"Communication style: {tone_description}. "
        f"Message style: {length_description}. "
        f"Tone: {caps_description}. "
        f"{emoji_description}"
    )
    
    logger.info(f"âœ… Style extracted: {style_desc}")
    
    return style_desc


# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
_mistral_pipeline = None
_mistral_model = None
_mistral_tokenizer = None
_device = None
_dtype = None


def _get_device_and_dtype():
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ (GPU/CPU) Ğ¸ dtype"""
    global _device, _dtype
    
    if _device is not None:
        return _device, _dtype
    
    if TRANSFORMERS_AVAILABLE and torch.cuda.is_available():
        _device = "cuda"
        _dtype = torch.float16
        logger.info("âœ… GPU Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ CUDA Ñ float16")
    else:
        _device = "cpu"
        _dtype = torch.float32
        logger.info("âš ï¸ GPU Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ CPU Ñ float32")
    
    return _device, _dtype


def _load_mistral_model():
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mistral-7B-Instruct Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ GPU/CPU"""
    global _mistral_pipeline, _mistral_model, _mistral_tokenizer
    
    if _mistral_pipeline is not None:
        return _mistral_pipeline
    
    if not TRANSFORMERS_AVAILABLE:
        logger.warning("Transformers Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°")
        return None
    
    try:
        device, dtype = _get_device_and_dtype()
        model_name = "mistralai/Mistral-7B-Instruct-v0.1"
        
        logger.info(f"Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ {model_name} Ğ½Ğ° {device} Ñ dtype {dtype}...")
        
        try:
            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                use_fast=False
            )
            
            # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ pad_token ĞµÑĞ»Ğ¸ ĞµĞ³Ğ¾ Ğ½ĞµÑ‚
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            logger.info("âœ… Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½")
            
        except Exception as e:
            logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Mistral: {e}, Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ")
            # Fallback Ğ½Ğ° DialoGPT ĞµÑĞ»Ğ¸ Mistral Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
            try:
                model_name = "microsoft/DialoGPT-medium"
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                logger.info(f"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {model_name}")
            except Exception as e2:
                logger.error(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€: {e2}")
                return None
        
        try:
            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ dtype
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=dtype,
                device_map="auto" if device == "cuda" else None,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            
            # ĞŸĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµĞ¼ Ğ½Ğ° CPU ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
            if device == "cpu":
                model = model.to(device)
            
            logger.info(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ½Ğ° {device}")
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ pipeline
            _mistral_pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=0 if device == "cuda" else -1
            )
            
            _mistral_model = model
            _mistral_tokenizer = tokenizer
            
            logger.info("âœ… Pipeline ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾")
            return _mistral_pipeline
            
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
            # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            if model_name != "microsoft/DialoGPT-medium":
                try:
                    logger.info("ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ DialoGPT-medium ĞºĞ°Ğº fallback...")
                    model_name = "microsoft/DialoGPT-medium"
                    model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        torch_dtype=dtype,
                        device_map="auto" if device == "cuda" else None
                    )
                    if device == "cpu":
                        model = model.to(device)
                    
                    _mistral_pipeline = pipeline(
                        "text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        device=0 if device == "cuda" else -1
                    )
                    _mistral_model = model
                    _mistral_tokenizer = tokenizer
                    logger.info("âœ… ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°")
                    return _mistral_pipeline
                except Exception as e2:
                    logger.error(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {e2}")
            return None
        
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mistral: {e}", exc_info=True)
        return None


def generate_reply(fan_msg: str, style_desc: str = "") -> str:
    """
    Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ½Ğ°Ñ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM (Mistral-7B-Instruct)
    
    Args:
        fan_msg: Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ñ„Ğ°Ğ½Ğ°Ñ‚Ğ°
        style_desc: ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾). 
                   Ğ•ÑĞ»Ğ¸ Ğ¿ÑƒÑÑ‚Ğ¾Ğµ Ğ¸Ğ»Ğ¸ "No style data available", Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ fallback persona
        
    Returns:
        Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ (short, engaging reply)
    """
    try:
        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° prompt ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼
        if style_desc and style_desc.strip() and style_desc != "No style data available":
            prompt = (
                f"You are a flirty 20s model. Style: {style_desc}. "
                f"Fan: {fan_msg}\nReply short, engaging:"
            )
        else:
            # Fallback persona ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚ style: "confident playful with ğŸ˜ğŸ’‹"
            prompt = (
                f"You are a flirty 20s model. Style: confident playful with ğŸ˜ğŸ’‹. "
                f"Fan: {fan_msg}\nReply short, engaging:"
            )
        
        logger.debug(f"Prompt: {prompt[:150]}...")
        
        # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Mistral Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        pipeline_obj = _load_mistral_model()
        
        if pipeline_obj:
            try:
                # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Mistral-7B-Instruct
                # max_tokens=50, temp=0.7 (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ max_new_tokens Ğ´Ğ»Ñ transformers)
                generation_kwargs = {
                    "max_new_tokens": 50,  # max_tokens=50
                    "temperature": 0.7,     # temp=0.7
                    "do_sample": True,
                    "return_full_text": False,
                }
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ pad_token_id ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½
                if _mistral_tokenizer and _mistral_tokenizer.pad_token_id:
                    generation_kwargs["pad_token_id"] = _mistral_tokenizer.pad_token_id
                
                outputs = pipeline_obj(prompt, **generation_kwargs)
                
                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
                if outputs and len(outputs) > 0:
                    reply = outputs[0].get('generated_text', '').strip()
                    
                    # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ prompt
                    reply = reply.split('\n')[0].strip()
                    
                    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸ĞµÑÑ Ñ‡Ğ°ÑÑ‚Ğ¸ prompt ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
                    if "Reply short, engaging:" in reply:
                        reply = reply.split("Reply short, engaging:")[-1].strip()
                    if "Fan:" in reply:
                        reply = reply.split("Fan:")[-1].strip()
                    
                    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñƒ
                    reply = ' '.join(reply.split())
                    if len(reply) > 200:  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸
                        reply = reply[:200].rsplit(' ', 1)[0] + "..."
                    
                    if reply:
                        logger.info(f"âœ… Generated reply: {reply[:100]}...")
                        return reply
                    else:
                        logger.warning("ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fallback")
                
            except Exception as e:
                logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Mistral: {e}, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fallback")
                import traceback
                logger.debug(traceback.format_exc())
        
        # Fallback: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
        logger.debug("Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fallback Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°")
        return _generate_fallback_reply(fan_msg)
        
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² generate_reply: {e}", exc_info=True)
        return _generate_fallback_reply(fan_msg)


def _generate_fallback_reply(fan_msg: str) -> str:
    """Fallback Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ±ĞµĞ· LLM"""
    fan_lower = fan_msg.lower()
    
    # ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²
    flirty_responses = [
        "Hey there! ğŸ˜˜ Thanks for the message!",
        "You're so sweet! ğŸ’•",
        "Aww, thank you! ğŸ¥°",
        "You're amazing! ğŸ”¥",
        "Thanks babe! ğŸ˜",
        "You're so hot! ğŸ’‹",
        "Hey gorgeous! ğŸ˜",
        "Thanks for reaching out! âœ¨",
        "You're perfect! â¤ï¸",
        "So glad to hear from you! ğŸ’–"
    ]
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
    if any(word in fan_lower for word in ['hi', 'hello', 'hey']):
        return "Hey there! ğŸ˜˜ How are you doing?"
    elif any(word in fan_lower for word in ['beautiful', 'gorgeous', 'sexy', 'hot']):
        return "Aww, thank you so much! ğŸ¥° You're so sweet!"
    elif any(word in fan_lower for word in ['love', 'like']):
        return "You're amazing! ğŸ’• Thanks for the support!"
    else:
        import random
        return random.choice(flirty_responses)


if __name__ == "__main__":
    # Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
    print("=" * 60)
    print("Testing AI functions")
    print("=" * 60)
    
    # Ğ¢ĞµÑÑ‚ extract_style
    print("\n1. Testing extract_style...")
    sample_replies = [
        "Hi there! ğŸ˜Š How are you doing today?",
        "You're so beautiful! ğŸ’•",
        "Thanks for your message! âœ¨",
        "I'd love to chat more! ğŸ˜˜",
        "You're amazing! ğŸ”¥"
    ]
    style = extract_style(sample_replies)
    print(f"Style: {style}")
    
    # Ğ¢ĞµÑÑ‚ generate_reply
    print("\n2. Testing generate_reply...")
    test_msg = "hey gorgeous"
    test_style = "flirty with ğŸ˜˜"
    reply = generate_reply(test_msg, test_style)
    print(f"Fan message: {test_msg}")
    print(f"Style: {test_style}")
    print(f"Generated reply: {reply}")
    
    # Ğ¢ĞµÑÑ‚ Ğ±ĞµĞ· style (fallback persona)
    print("\n3. Testing generate_reply without style...")
    reply2 = generate_reply("hello there")
    print(f"Fan message: hello there")
    print(f"Generated reply (fallback): {reply2}")
